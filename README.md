

```
# ğŸ›ï¸ MetaSynAI

**MetaSynAI** is an AI-powered accessibility framework that combines **voice commands**, **hand gestures**, and **eye-tracking** for futuristic and inclusive user interaction.

---

## âœ¨ Features

- ğŸ—£ï¸ **Voice Assistant** â€“ Control applications via voice commands.
- âœ‹ **Hand Gesture Recognition** â€“ Perform tasks using hand movements.
- ğŸ‘ï¸ **Eye-Tracking Navigation** â€“ Interact using eye gaze.
- ğŸŒ **Modern Web Interface** â€“ Sleek, responsive, and interactive UI.

---

## ğŸ› ï¸ Tech Stack

- **Frontend**: HTML, CSS, JavaScript  
- **Backend**: Python (Flask)  
- **ML/AI**: TensorFlow, OpenCV  
- **Voice**: SpeechRecognition API  
- **Eye-Tracking**: Dlib, OpenCV  

---

## ğŸ“ Folder Structure

```

MetaSynAI/
â”œâ”€â”€ assets/
â”œâ”€â”€ css/
â”œâ”€â”€ js/
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ voice-assistant.html
â”‚   â”œâ”€â”€ hand-gestures.html
â”‚   â””â”€â”€ eye-gaze.html
â”œâ”€â”€ app.py
â”œâ”€â”€ gesture\_zoom.py
â”œâ”€â”€ voice-assistant-server.js
â”œâ”€â”€ README.md

````

---

## âš™ï¸ Getting Started

### 1. Clone the repo
```bash
git clone https://github.com/dj-ayush/MetaSynAI.git
cd MetaSynAI
````

### 2. Create & activate a virtual environment

```bash
python -m venv venv
# On Windows
venv\Scripts\activate
# On Mac/Linux
source venv/bin/activate
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### 4. Run the app

```bash
python app.py
```

### 5. Open in browser

```
http://localhost:5000
```

---

## ğŸ¤ Contributing

We welcome contributions!

1. Fork the repo
2. Create a new branch: `git checkout -b feature-name`
3. Commit your changes: `git commit -m "Added feature"`
4. Push to your branch: `git push origin feature-name`
5. Create a pull request ğŸš€

---

## ğŸ“„ License

This project is licensed under the [MIT License](LICENSE).

---

> Built with â¤ï¸ by [@dj-ayush](https://github.com/dj-ayush)

```
